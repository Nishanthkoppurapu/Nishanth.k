{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Full Explanation of the Sentence Transformer Implementation (PyTorch)**\n",
        "\n",
        "This script implements a **Sentence Transformer** model using **PyTorch** and **Hugging Face's `transformers` library**. The model encodes input sentences into fixed-length embeddings using a **pre-trained transformer** (like BERT) and applies **mean pooling** to aggregate token representations into a single sentence vector.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BJqaacA6vhY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Required Libraries.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "6IePSt3jvxMV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `torch` and `torch.nn` are used for defining and handling the PyTorch model.\n",
        "- `AutoModel` and `AutoTokenizer` from Hugging Face are used to load a **pre-trained transformer model** and its corresponding tokenizer."
      ],
      "metadata": {
        "id": "O3JjN3kLvyBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- The class `SentenceTransformer` inherits from `nn.Module`, making it a PyTorch model.\n",
        "- `AutoModel.from_pretrained(model_name)` loads a **pre-trained transformer model** (`bert-base-uncased` by default).\n",
        "- `AutoTokenizer.from_pretrained(model_name)` loads the **corresponding tokenizer** for text processing."
      ],
      "metadata": {
        "id": "PEdzyyCvwIVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Transformer models like BERT generate **token embeddings** (one vector per token).\n",
        "- We need a **fixed-size sentence embedding**.\n",
        "- **Mean pooling** computes the average of all token embeddings, weighted by the **attention mask** (to ignore padding tokens).\n"
      ],
      "metadata": {
        "id": "oXJFzvg4wVFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Sentence Transformer Model.\n",
        "\n",
        "class SentenceTransformer(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
        "        super(SentenceTransformer, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Implementing Mean Pooling.\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        \"\"\"Compute mean pooling over token embeddings based on attention mask\"\"\"\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)  # Avoid division by zero\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    # Forward Pass (Encoding Sentences)\n",
        "    def forward(self, sentences):\n",
        "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        outputs = self.encoder(**inputs)\n",
        "        sentence_embeddings = self.mean_pooling(outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
        "        return sentence_embeddings"
      ],
      "metadata": {
        "id": "93nreaWfh44K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step-by-Step Breakdown:**\n",
        "1. **Expand the Attention Mask:**  \n",
        "   - `attention_mask.unsqueeze(-1).expand(token_embeddings.size())`  \n",
        "   - This ensures padding tokens don't contribute to the mean.\n",
        "2. **Multiply Token Embeddings by Attention Mask:**  \n",
        "   - This nullifies embeddings for padding tokens.\n",
        "3. **Sum the Token Embeddings Along Dimension 1:**  \n",
        "   - Computes the total sum of valid token embeddings.\n",
        "4. **Compute the Mean:**  \n",
        "   - Divide by the **sum of the attention mask** to get the **average** embedding."
      ],
      "metadata": {
        "id": "lgJ1qJi1wppM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Processing Input Sentences:**\n",
        "1. **Tokenization:**\n",
        "   - `self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")`\n",
        "   - Converts input text into tokenized format compatible with the transformer.\n",
        "   - `padding=True` ensures uniform input length.\n",
        "   - `truncation=True` prevents excessively long inputs.\n",
        "   - `return_tensors=\"pt\"` returns **PyTorch tensors**.\n",
        "\n",
        "2. **Passing Through Transformer:**\n",
        "   - `outputs = self.encoder(**inputs)`\n",
        "   - Generates **contextualized token embeddings** from the transformer model.\n",
        "\n",
        "3. **Applying Mean Pooling:**\n",
        "   - Converts token embeddings into a **single sentence embedding**.\n",
        "\n",
        "4. **Returning the Sentence Embeddings:**\n",
        "   - The output is a **fixed-length representation** of the sentence."
      ],
      "metadata": {
        "id": "wapP8sgMwwjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Initialization and Testing\n",
        "sentence_transformer = SentenceTransformer()\n",
        "sample_sentences = [\"This is a test sentence.\", \"Sentence transformers generate embeddings.\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = sentence_transformer(sample_sentences)\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)  # Expected: (batch_size, hidden_size)\n",
        "print(\"Sample Embeddings:\", embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKgK86hFw4aN",
        "outputId": "195f0e6c-bbdc-4989-ec15-fd2a4c589492"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: torch.Size([2, 768])\n",
            "Sample Embeddings: tensor([[ 6.6063e-02, -2.1769e-01, -1.5390e-01,  ..., -2.2918e-01,\n",
            "         -3.9263e-04,  3.9901e-01],\n",
            "        [ 6.9934e-02, -3.2461e-01, -2.9427e-01,  ..., -1.5882e-01,\n",
            "         -5.1646e-01,  1.5244e-01]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}